{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biol 359A | Statistical Tests: Linear Regression\n",
    "### Spring 2025, Week 3\n",
    "Objectives:\n",
    "- Interact with real data\n",
    "- Learn how to fit lines to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, Layout, Dropdown\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind, f, f_oneway\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import statsmodels.stats.multicomp as mc\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd, MultiComparison\n",
    "import sklearn as sk\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r week3_anova/\n",
    "! git clone https://github.com/BIOL359A-FoundationsOfQBio-Spr24/week3_anova.git\n",
    "! cp -r week3_anova/* .\n",
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For today's lesson we will be working on real breast cancer data from the[ Wisconsin Diagnostic Breast Cancer Database (WDBC)](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic).\n",
    "\n",
    "Here is a summary of the data from the data source:\n",
    "```\n",
    "\tFeatures are computed from a digitized image of a fine needle\n",
    "\taspirate (FNA) of a breast mass.  They describe\n",
    "\tcharacteristics of the cell nuclei present in the image.\n",
    "\tA few of the images can be found at\n",
    "\thttp://www.cs.wisc.edu/~street/images/\n",
    "\n",
    "\tSeparating plane described above was obtained using\n",
    "\tMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
    "\tConstruction Via Linear Programming.\" Proceedings of the 4th\n",
    "\tMidwest Artificial Intelligence and Cognitive Science Society,\n",
    "\tpp. 97-101, 1992], a classification method which uses linear\n",
    "\tprogramming to construct a decision tree.  Relevant features\n",
    "\twere selected using an exhaustive search in the space of 1-4\n",
    "\tfeatures and 1-3 separating planes.\n",
    "\n",
    "\tThe actual linear program used to obtain the separating plane\n",
    "\tin the 3-dimensional space is that described in:\n",
    "\t[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
    "\tProgramming Discrimination of Two Linearly Inseparable Sets\",\n",
    "\tOptimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "\tThis database is also available through the UW CS ftp server:\n",
    "\tftp ftp.cs.wisc.edu\n",
    "\tcd math-prog/cpo-dataset/machine-learn/WDBC/\n",
    "    \n",
    "    Source:\n",
    "    W.N. Street, W.H. Wolberg and O.L. Mangasarian\n",
    "\tNuclear feature extraction for breast tumor diagnosis.\n",
    "\tIS&T/SPIE 1993 International Symposium on Electronic Imaging: Science\n",
    "\tand Technology, volume 1905, pages 861-870, San Jose, CA, 1993.\n",
    "```\n",
    "\n",
    "What do all the column names mean?\n",
    "\n",
    "- ID number\n",
    "- Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry\n",
    "- fractal dimension (\"coastline approximation\" - 1) - a measure of \"complexity\" of a 2D image.\n",
    "\n",
    "\n",
    "Cateogory Distribution: 357 benign, 212 malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clean_data\n",
    "\n",
    "cancer_dataset = clean_data.generate_clean_dataframe()\n",
    "cancer_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting lines to data\n",
    "\n",
    "\n",
    "Fitting lines to data, or linear regression, learns the best linear relationship between two feature (X) and outcome (Y) by minimizing the sum of squared errors (SSE) between predicted and actual values. This fitted line can then be used to predict the value of one variable given the other.\n",
    "\n",
    "Additionally, it helps quantify the strength and direction of the relationship (via the slope), and can provide insights into how changes in X are associated with changes in Y.\n",
    "\n",
    "\\begin{align*}\n",
    "S_{xy} &= \\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x}) \\\\\n",
    "S_{xx} &= \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\\\\n",
    "\\\\\n",
    "\\text{Coefficients are:} \\\\\n",
    "\\hat{\\beta}_1 &= \\frac{S_{xy}}{S_{xx}} \\\\\n",
    "\\hat{\\beta}_0 &= \\mathbb{E}[Y] - \\hat{\\beta}_1 \\mathbb{E}[X]\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_beta_coefficients(x, y):\n",
    "    \"\"\"\n",
    "    Calculate beta coefficients using least squares method.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): Independent variable values\n",
    "    y (array-like): Dependent variable values\n",
    "\n",
    "    Returns:\n",
    "    tuple: (beta_0, beta_1, S_xy, S_xx)\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays if they aren't already\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Calculate means\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    S_xy = np.sum((y - y_mean) * (x - x_mean))\n",
    "    S_xx = np.sum((x - x_mean) ** 2)\n",
    "\n",
    "    beta_1 = S_xy / S_xx\n",
    "    beta_0 = y_mean - beta_1 * x_mean\n",
    "\n",
    "    return beta_0, beta_1, S_xy, S_xx\n",
    "\n",
    "def plot_regression(x, y, beta_0, beta_1, feature_name, outcome_name):\n",
    "    \"\"\"\n",
    "    Plot scatter points and the fitted regression line.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): Independent variable values\n",
    "    y (array-like): Dependent variable values\n",
    "    beta_0 (float): Intercept coefficient\n",
    "    beta_1 (float): Slope coefficient\n",
    "    feature_name (str): Name of the independent variable\n",
    "    outcome_name (str): Name of the dependent variable\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot scatter points\n",
    "    plt.scatter(x, y, color='blue', alpha=0.6, label='Data points')\n",
    "\n",
    "    # Plot regression line\n",
    "    x_range = np.linspace(min(x), max(x), 100)\n",
    "    y_pred = beta_0 + beta_1 * x_range\n",
    "    plt.plot(x_range, y_pred, color='red', linewidth=2, label=f'Fitted line: y = {beta_0:.4f} + {beta_1:.4f}x')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel(outcome_name)\n",
    "    plt.title(f'Linear Regression: {outcome_name} vs {feature_name}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    # Display equation on the plot\n",
    "    equation = f\"{outcome_name} = {beta_0:.4f} + {beta_1:.4f} × {feature_name}\"\n",
    "    plt.annotate(equation, xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with your dataset\n",
    "def run_regression_analysis(data, feature_name, outcome_name):\n",
    "    \"\"\"\n",
    "    Run the complete regression analysis on the given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    data (DataFrame): Pandas DataFrame containing the data\n",
    "    feature_name (str): Name of the independent variable column\n",
    "    outcome_name (str): Name of the dependent variable column\n",
    "\n",
    "    Returns:\n",
    "    tuple: (beta_0, beta_1)\n",
    "    \"\"\"\n",
    "    # Extract the variables\n",
    "    x = data[feature_name]\n",
    "    y = data[outcome_name]\n",
    "\n",
    "    # Calculate beta coefficients\n",
    "    beta_0, beta_1, S_xy, S_xx = calculate_beta_coefficients(x, y)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Covariance (S_xy): {S_xy:.4f}\")\n",
    "    print(f\"Variance of X (S_xx): {S_xx:.4f}\")\n",
    "    print(f\"Beta 1 (slope): {beta_1:.4f}\")\n",
    "    print(f\"Beta 0 (intercept): {beta_0:.4f}\")\n",
    "    print(f\"Regression equation: {outcome_name} = {beta_0:.4f} + {beta_1:.4f} × {feature_name}\")\n",
    "\n",
    "    # Plot the results\n",
    "    plot_regression(x, y, beta_0, beta_1, feature_name, outcome_name)\n",
    "\n",
    "    return beta_0, beta_1\n",
    "feature_name = 'mean_radius'\n",
    "outcome_name = 'mean_perimeter'\n",
    "beta_0, beta_1 = run_regression_analysis(cancer_dataset, feature_name, outcome_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
