{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biol 359A | Statistical Tests: Linear Regression\n",
    "### Spring 2025, Week 3\n",
    "Objectives:\n",
    "- Interact with real data\n",
    "- Learn how to fit lines to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from ipywidgets import interact, IntSlider, FloatSlider, Layout, Dropdown\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_ind, f, f_oneway\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import statsmodels.stats.multicomp as mc\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd, MultiComparison\n",
    "import sklearn as sk\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r week3_linearRegression/\n",
    "! git clone https://github.com/BIOL359A-FoundationsOfQBio-Spr25/week3_linearRegression.git\n",
    "! cp -r week3_linearRegression/* .\n",
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For today's lesson we will be working on real breast cancer data from the[ Wisconsin Diagnostic Breast Cancer Database (WDBC)](https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic).\n",
    "\n",
    "Here is a summary of the data from the data source:\n",
    "```\n",
    "\tFeatures are computed from a digitized image of a fine needle\n",
    "\taspirate (FNA) of a breast mass.  They describe\n",
    "\tcharacteristics of the cell nuclei present in the image.\n",
    "\tA few of the images can be found at\n",
    "\thttp://www.cs.wisc.edu/~street/images/\n",
    "\n",
    "\tSeparating plane described above was obtained using\n",
    "\tMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
    "\tConstruction Via Linear Programming.\" Proceedings of the 4th\n",
    "\tMidwest Artificial Intelligence and Cognitive Science Society,\n",
    "\tpp. 97-101, 1992], a classification method which uses linear\n",
    "\tprogramming to construct a decision tree.  Relevant features\n",
    "\twere selected using an exhaustive search in the space of 1-4\n",
    "\tfeatures and 1-3 separating planes.\n",
    "\n",
    "\tThe actual linear program used to obtain the separating plane\n",
    "\tin the 3-dimensional space is that described in:\n",
    "\t[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
    "\tProgramming Discrimination of Two Linearly Inseparable Sets\",\n",
    "\tOptimization Methods and Software 1, 1992, 23-34].\n",
    "\n",
    "\tThis database is also available through the UW CS ftp server:\n",
    "\tftp ftp.cs.wisc.edu\n",
    "\tcd math-prog/cpo-dataset/machine-learn/WDBC/\n",
    "    \n",
    "    Source:\n",
    "    W.N. Street, W.H. Wolberg and O.L. Mangasarian\n",
    "\tNuclear feature extraction for breast tumor diagnosis.\n",
    "\tIS&T/SPIE 1993 International Symposium on Electronic Imaging: Science\n",
    "\tand Technology, volume 1905, pages 861-870, San Jose, CA, 1993.\n",
    "```\n",
    "\n",
    "What do all the column names mean?\n",
    "\n",
    "- ID number\n",
    "- Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry\n",
    "- fractal dimension (\"coastline approximation\" - 1) - a measure of \"complexity\" of a 2D image.\n",
    "\n",
    "\n",
    "Cateogory Distribution: 357 benign, 212 malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clean_data\n",
    "\n",
    "cancer_dataset = clean_data.generate_clean_dataframe()\n",
    "cancer_dataset = cancer_dataset.reset_index()\n",
    "cancer_dataset.drop(\"ID\", axis=1, inplace=True)\n",
    "cancer_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In order for us to use diagnosis as a variable, we need to turn it from categorical (M, B) into numerical values. We will set \"M\" to 1, and \"B\" to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset['diagnosis'].replace([\"M\",\"B\"], [1,0], inplace=True)\n",
    "cancer_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting lines to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple linear regression\n",
    "\n",
    "\n",
    "Fitting lines to data, or linear regression, learns the best linear relationship between two feature (X) and outcome (Y) by minimizing the sum of squared errors (SSE) between predicted and actual values. This fitted line can then be used to predict the value of one variable given the other.\n",
    "\n",
    "Additionally, it helps quantify the strength and direction of the relationship (via the slope), and can provide insights into how changes in X are associated with changes in Y.\n",
    "\n",
    "\\begin{align*}\n",
    "S_{xy} &= \\sum_{i=1}^{n} (y_i - \\bar{y})(x_i - \\bar{x}) \\\\\n",
    "S_{xx} &= \\sum_{i=1}^{n} (x_i - \\bar{x})^2 \\\\\n",
    "\\\\\n",
    "\\text{Coefficients are:} \\\\\n",
    "\\hat{\\beta}_1 &= \\frac{S_{xy}}{S_{xx}} \\\\\n",
    "\\hat{\\beta}_0 &= \\mathbb{E}[Y] - \\hat{\\beta}_1 \\mathbb{E}[X]\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_beta_coefficients(x, y):\n",
    "    \"\"\"\n",
    "    Calculate beta coefficients using least squares method.\n",
    "\n",
    "    Parameters:\n",
    "    x (array-like): Independent variable values\n",
    "    y (array-like): Dependent variable values\n",
    "\n",
    "    Returns:\n",
    "    tuple: (beta_0, beta_1, S_xy, S_xx)\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays if they aren't already\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Calculate means\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    S_xy = np.sum((y - y_mean) * (x - x_mean))\n",
    "    S_xx = np.sum((x - x_mean) ** 2)\n",
    "    S_yy = np.sum((y - y_mean) ** 2)\n",
    "\n",
    "    beta_1 = S_xy / S_xx\n",
    "    beta_0 = y_mean - beta_1 * x_mean\n",
    "\n",
    "    return beta_0, beta_1, S_xy, S_xx, S_yy\n",
    "\n",
    "@widgets.interact(x_feature=list(cancer_dataset), y_feature=list(cancer_dataset))\n",
    "def make_interactive_regression(x_feature=\"mean_radius\", y_feature=\"mean_perimeter\"):\n",
    "    \"\"\"\n",
    "    Interactive function to run regression analysis on selected features using decorator syntax.\n",
    "    \n",
    "    Parameters:\n",
    "    x_feature (str): Name of the independent variable column\n",
    "    y_feature (str): Name of the dependent variable column\n",
    "    \"\"\"\n",
    "    # Extract the variables\n",
    "    x = cancer_dataset[x_feature]\n",
    "    y = cancer_dataset[y_feature]\n",
    "    \n",
    "    # Calculate beta coefficients\n",
    "    beta_0, beta_1, S_xy, S_xx, S_yy = calculate_beta_coefficients(x, y)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot scatter points\n",
    "    plt.scatter(x, y, color='blue', alpha=0.6, label='Data points')\n",
    "    \n",
    "    # Plot regression line\n",
    "    x_range = np.linspace(min(x), max(x), 100)\n",
    "    y_pred = beta_0 + beta_1 * x_range\n",
    "    plt.plot(x_range, y_pred, color='red', linewidth=2, \n",
    "             label=f'Fitted line: y = {beta_0:.4f} + {beta_1:.4f}x')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(x_feature)\n",
    "    plt.ylabel(y_feature)\n",
    "    plt.title(f'Linear Regression: {y_feature} vs {x_feature}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Display equation and statistics on the plot\n",
    "    equation = f\"{y_feature} = {beta_0:.4f} + {beta_1:.4f} × {x_feature}\"\n",
    "    stats = f\"S_xy: {S_xy:.4f}, S_xx: {S_xx:.4f}\"\n",
    "    \n",
    "    plt.annotate(equation, xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.annotate(stats, xy=(0.05, 0.89), xycoords='axes fraction',\n",
    "                 fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"S_xy: {S_xy:.4f}\")\n",
    "    print(f\"S_xx: {S_xx:.4f}\")\n",
    "    print(f\"Correlation coefficient: {S_xy/np.sqrt(S_xx*S_yy)}\")\n",
    "    print(f\"Beta 1 (slope): {beta_1:.4f}\")\n",
    "    print(f\"Beta 0 (intercept): {beta_0:.4f}\")\n",
    "    print(f\"Regression equation: {y_feature} = {beta_0:.4f} + {beta_1:.4f} × {x_feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for linear regression\n",
    "\n",
    "We are going to introduce the concept of the __Coefficient of Determination__: $R^2$. \n",
    "\n",
    "$$ R^2 = \\frac{SSR}{SST} = 1 - \\frac{SSE}{SST}$$\n",
    "\n",
    "Where \n",
    "- SSE = error of sum of squares\n",
    "- SSR = regression sum of squares\n",
    "- SST = total sum of squares\n",
    "\n",
    "This value conceptually represents the \"amount of variance in y is explained by x\" and can be thought of as how well the model fits the training data. This is a seperate concept to the Pearson Correlation Coefficient, $\\rho$, which in the cases of a __strictly linear regression__, $R^2 = \\rho^2$. \n",
    "\n",
    "You can also calculate the adjusted $R^2$ which accounts for the model size and parameters. This way, you can compare the adjusted $R^2$ of simple models to the adjusted $R^2$ of complex models, which you can't necessarily do if you just use  $R^2$. \n",
    "\n",
    "$$R^2_{adj} = 1 - \\frac{\\frac{SSE}{n-k-1}}{\\frac{SST}{n - 1}}$$\n",
    "\n",
    "Where \n",
    "- n = number of data points\n",
    "- k number of independent variables\n",
    "\n",
    "Generally speaking, $R^2$ is used to characterize the fit of a model, where as $\\rho$ is used to characterize the relationship between two variables. \n",
    "\n",
    "We will perform a simple linear regression between the response variable, y, and the feature, x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_r_squared(x, y, beta_0, beta_1):\n",
    "    \"\"\"\n",
    "    Calculate the coefficient of determination (R²).\n",
    "    \n",
    "    Parameters:\n",
    "    x (array-like): Independent variable values\n",
    "    y (array-like): Dependent variable values\n",
    "    beta_0 (float): Intercept coefficient\n",
    "    beta_1 (float): Slope coefficient\n",
    "    \n",
    "    Returns:\n",
    "    float: R² value\n",
    "    \"\"\"\n",
    "    # Convert to numpy arrays if they aren't already\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Calculate predicted values\n",
    "    y_pred = beta_0 + beta_1 * x\n",
    "    \n",
    "    # Calculate total sum of squares (TSS)\n",
    "    y_mean = np.mean(y)\n",
    "    TSS = np.sum((y - y_mean) ** 2)\n",
    "    \n",
    "    # Calculate residual sum of squares (RSS)\n",
    "    RSS = np.sum((y - y_pred) ** 2)\n",
    "    \n",
    "    # Calculate R-squared\n",
    "    r_squared = 1 - (RSS / TSS)\n",
    "    \n",
    "    return r_squared\n",
    "\n",
    "@widgets.interact(x_feature=list(cancer_dataset), y_feature=list(cancer_dataset))\n",
    "def make_interactive_regression(x_feature=\"mean_radius\", y_feature=\"mean_perimeter\"):\n",
    "    \"\"\"\n",
    "    Interactive function to run regression analysis on selected features using decorator syntax.\n",
    "    \n",
    "    Parameters:\n",
    "    x_feature (str): Name of the independent variable column\n",
    "    y_feature (str): Name of the dependent variable column\n",
    "    \"\"\"\n",
    "    # Extract the variables\n",
    "    x = cancer_dataset[x_feature]\n",
    "    y = cancer_dataset[y_feature]\n",
    "    \n",
    "    # Calculate beta coefficients\n",
    "    beta_0, beta_1, S_xy, S_xx, S_yy = calculate_beta_coefficients(x, y)\n",
    "    \n",
    "    # Calculate R-squared\n",
    "    r_squared = calculate_r_squared(x, y, beta_0, beta_1)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot scatter points\n",
    "    plt.scatter(x, y, color='blue', alpha=0.6, label='Data points')\n",
    "    \n",
    "    # Plot regression line\n",
    "    x_range = np.linspace(min(x), max(x), 100)\n",
    "    y_pred = beta_0 + beta_1 * x_range\n",
    "    plt.plot(x_range, y_pred, color='red', linewidth=2, \n",
    "             label=f'Fitted line: y = {beta_0:.4f} + {beta_1:.4f}x')\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(x_feature)\n",
    "    plt.ylabel(y_feature)\n",
    "    plt.title(f'Linear Regression: {y_feature} vs {x_feature}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Display equation and statistics on the plot\n",
    "    equation = f\"{y_feature} = {beta_0:.4f} + {beta_1:.4f} × {x_feature}\"\n",
    "    stats = f\"R² = {r_squared:.4f}, S_xy: {S_xy:.4f}, S_xx: {S_xx:.4f}\"\n",
    "    \n",
    "    plt.annotate(equation, xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.annotate(stats, xy=(0.05, 0.89), xycoords='axes fraction',\n",
    "                 fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"S_xy: {S_xy:.4f}\")\n",
    "    print(f\"S_xx: {S_xx:.4f}\")\n",
    "    print(f\"Correlation coefficient: {S_xy/np.sqrt(S_xx*S_yy)}\")\n",
    "    print(f\"Beta 1 (slope): {beta_1:.4f}\")\n",
    "    print(f\"Beta 0 (intercept): {beta_0:.4f}\")\n",
    "    print(f\"R-squared: {r_squared:.4f}\")\n",
    "    print(f\"Regression equation: {y_feature} = {beta_0:.4f} + {beta_1:.4f} × {x_feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3!\n",
    "[template](https://colab.research.google.com/drive/1dhnxVqRQli5bQPGIQl3RnzCer0X94vGS?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression \n",
    "\n",
    "We have other variables that we can use to explore the relationships in this dataset. We can use multiple linear regression to include more variables, where we are drawing a multi-dimensional shape, rather than simply a line, as our model. The plot below is currently the same as the one we looked at above with mean_area and mean_radius. Try adding some other variables into your regression by checking the boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE_FONT = 20\n",
    "LABEL_FONT = 16\n",
    "TICK_FONT = 16\n",
    "FIG_SIZE = (12,12)\n",
    "COLORS= [\"#008080\",\"#CA562C\"]\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"whitegrid\",  {'axes.linewidth': 2, 'axes.edgecolor':'black'})\n",
    "sns.set(font_scale=1, rc={'figure.figsize':FIG_SIZE}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(df, feature_cols, response_col, standardized = False):\n",
    "    \"\"\"\n",
    "    Use linear_model to run a linear regression using sklearn\n",
    "    \n",
    "    \"\"\"\n",
    "    X = df[feature_cols]\n",
    "    y = df[response_col]\n",
    "    if standardized:\n",
    "        X = StandardScaler().fit_transform(X)\n",
    "        y = StandardScaler().fit_transform(y.values.reshape(-1, 1))\n",
    "    regression = linear_model.LinearRegression() \n",
    "    regression.fit(X,y)\n",
    "    try:\n",
    "        print('Intercept of MLR model is {0:0.2f}'.format(regression.intercept_))\n",
    "    except TypeError:\n",
    "        print('Intercept of MLR model is {0:0.2f}'.format(regression.intercept_[0]))\n",
    "    print('Regression Coefficients: (coefficients for linear combinations)')\n",
    "    for feature, coef in zip(feature_cols, regression.coef_.flatten()):\n",
    "        print(f'{feature} ~ {coef:.2f}')\n",
    "    return regression.predict(X), regression.score(X,y)\n",
    "    \n",
    "def parity_plot(true, pred, r_squared=None, title='', hue=None):\n",
    "    \"\"\"\n",
    "    plot true vs the predicted data\n",
    "    inputs: 2 list-like (arrays) data structures\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1,1,figsize=(10, 8))\n",
    "    if hue is not None:\n",
    "        sns.scatterplot(x=true, y=pred, hue=hue)\n",
    "    else: \n",
    "        sns.scatterplot(x=true, y=pred)\n",
    "    min_value = min(min(true), min(pred))\n",
    "    max_value = max(max(true), max(pred))\n",
    "    plt.plot([min_value, max_value],[min_value, max_value], '--', label=\"parity\")\n",
    "    plt.xlabel('True Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    ax.set_box_aspect(1)\n",
    "    sns.despine()\n",
    "    plt.text(1.01, 0.98, r\"$R^2 = {0:.2f}$\".format(r_squared),\n",
    "         ha='left', va='top', size =LABEL_FONT,\n",
    "         transform=ax.transAxes)\n",
    "    plt.title('Parity Plot: {}'.format(title), size=TITLE_FONT)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_regression(feature_cols = \n",
    "                   ['mean_radius', \n",
    "                    'mean_texture', \n",
    "                    'mean_perimeter', \n",
    "                    'mean_smoothness', \n",
    "                    'mean_compactness', \n",
    "                    'mean_concavity', \n",
    "                    'mean_concave_points', \n",
    "                    'mean_symmetry', \n",
    "                    'mean_fractal_dimension'], \n",
    "                     response_col='mean_area',\n",
    "                     standardized=False,\n",
    "                     parity=True):\n",
    "    y_pred, r_squared = linear_regression(cancer_dataset, feature_cols, response_col, standardized = standardized)\n",
    "    if parity: parity_plot(cancer_dataset[response_col], y_pred.flatten(), r_squared, hue=cancer_dataset[\"diagnosis\"])\n",
    "\n",
    "\n",
    "@widgets.interact(response=list(cancer_dataset))   \n",
    "def regression_wrapper(response=\"mean_radius\",\n",
    "                       diagnosis=False,\n",
    "                       radius=False, \n",
    "                       texture=False, \n",
    "                       perimeter=False, \n",
    "                       area=True,\n",
    "                       smoothness=False, \n",
    "                       compactness=False, \n",
    "                       concavity=False, \n",
    "                       concave_points=False, \n",
    "                       symmetry=False, \n",
    "                       fractal_dimension=False):\n",
    "    features = []\n",
    "    if diagnosis: features.append(\"diagnosis\")\n",
    "    if radius: features.append(\"mean_radius\")\n",
    "    if texture: features.append(\"mean_texture\")\n",
    "    if perimeter: features.append(\"mean_perimeter\")\n",
    "    if area: features.append(\"mean_area\")\n",
    "    if smoothness: features.append(\"mean_smoothness\")\n",
    "    if compactness: features.append(\"mean_compactness\")    \n",
    "    if concavity: features.append(\"mean_concavity\")\n",
    "    if concave_points: features.append(\"mean_concave_points\")\n",
    "    if symmetry: features.append(\"mean_symmetry\")\n",
    "    if fractal_dimension: features.append(\"mean_fractal_dimension\")\n",
    "        \n",
    "\n",
    "    run_regression(feature_cols=features, response_col = response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
